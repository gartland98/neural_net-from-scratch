{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "\n",
    "class TwoLayerNet(object):\n",
    "  \"\"\"\n",
    "  two-layer-perceptron.\n",
    "  Input dimension : N\n",
    "  Hidden layer dimension : H\n",
    "  Output dimension : C\n",
    "\n",
    "\n",
    "  input - linear layer - ReLU - linear layer - output\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "    \"\"\"\n",
    "    initialise model with small random price of weight and bias initialised as 0.\n",
    "    weight and bias will be saved in dictionary called self.params\n",
    "\n",
    "    W1: weight of first layer; (D, H)\n",
    "    b1: bias of first layer; (H,)\n",
    "    W2: weight of second layer; (H, C)\n",
    "    b2: bias of second layer; (C,)\n",
    "\n",
    "    Inputs:\n",
    "    - input_size: dimension of input layer.\n",
    "    - hidden_size: number of neurons in hidden layer.\n",
    "    - output_size: output dimesion.\n",
    "    \"\"\"\n",
    "    self.params = {}\n",
    "    self.params['W1'] = std * torch.randn(input_size, hidden_size)\n",
    "    self.params['b1'] = torch.zeros(hidden_size)\n",
    "    self.params['W2'] = std * torch.randn(hidden_size, output_size)\n",
    "    self.params['b2'] = torch.zeros(output_size)\n",
    "\n",
    "  def loss(self, X, y=None):\n",
    "    \"\"\"\n",
    "    calculate the gradient and loss of neural network\n",
    "\n",
    "    Inputs:\n",
    "    - X: input data. shape (N, D). each x[i] is one training sample and total N number of sample given as input\n",
    "    - y: vector of training label. y[i] is the integer type label of x[i]\n",
    "      if y is given, return loss and gradient and if not return output\n",
    "\n",
    "    Returns:\n",
    "    if y is not given, return the score matrix which shape is (N,C)\n",
    "    scores[i, c] is the score of class c of input X[i]\n",
    "\n",
    "    if y is given, return the tuple: (loss, grads)\n",
    "    loss: loss (scalar) of training batch\n",
    "    grads: dictionary of {parameter name: gradient} (it should use same key with self.params)\n",
    "    \"\"\"\n",
    "    # call weight and bias in dictionary\n",
    "    W1, b1 = self.params['W1'], self.params['b1']\n",
    "    W2, b2 = self.params['W2'], self.params['b2']\n",
    "    N, D = X.size()\n",
    "\n",
    "    # calculate forward path\n",
    "    scores = None\n",
    "    #carry out forward path, save the value of output layer in the 'scores' (shape : (N, C))  #\n",
    "    #input - linear layer - ReLU - linear layer - output             #\n",
    "    \n",
    "    scores=None\n",
    "    hidden_layer1=torch.mm(X,W1)+b1\n",
    "    hidden_layer1_output=torch.nn.functional.relu(hidden_layer1)\n",
    "    output_layer=torch.mm(hidden_layer1_output,W2)+b2\n",
    "    scores=output_layer\n",
    "    \n",
    "    # if answer (target) is not given, return 'scores' and finish it\n",
    "    if y is None:\n",
    "      return scores\n",
    "\n",
    "    # calculate loss\n",
    "    loss = None\n",
    "    e = torch.exp(scores)\n",
    "    softmax = e / torch.sum(e, dim=1, keepdim=True)\n",
    "    \n",
    "    #  calculate the value of loss by using Output and save in 'loss' (scalar)\n",
    "    #\n",
    "    #       loss function : negative log likelihood                    \n",
    "    #       Use the value of softmax in the 'softmax' variable when calculating \n",
    "    #\n",
    "    #  y' indicates the answer index and apply - log to the answer probability         \n",
    "    \n",
    "    index=0\n",
    "    loss=0\n",
    "    while index<len(y):\n",
    "        for prob in softmax:\n",
    "            loss-=torch.log(prob[y[index]])\n",
    "            index+=1\n",
    "    number_of_data=X.shape[0]\n",
    "    loss=loss/number_of_data\n",
    "    \n",
    "\n",
    "\n",
    "    # Backward path(calculate Gradient) \n",
    "    grads = {}\n",
    "    # calculate the gradient of weight and bias and save in 'grads' dictionary   \n",
    "    # key of dictionary should be set same as self.params             \n",
    "    # grads['W1'] have to have same shape with self.params['W1']                     \n",
    "            \n",
    "    count=0\n",
    "    dsoftmax=softmax.clone()\n",
    "    while count<len(y):\n",
    "        for prob in dsoftmax:\n",
    "            prob[y[count]]-=1\n",
    "            count+=1\n",
    "    dsoftmax/=number_of_data\n",
    "    \n",
    "    \n",
    "    grads['W2'] = torch.mm(hidden_layer1_output.t(), dsoftmax)\n",
    "    grads['b2'] = torch.sum(dsoftmax, dim=0)\n",
    "        \n",
    "    dhidden_layer1_output = torch.mm(dsoftmax,W2.t())*(hidden_layer1_output>0).float() #dhidden_layer1_output --> dhidden_layer1\n",
    "    grads['W1'] = torch.mm(X.t(), dhidden_layer1_output)#dhidden_layer1_output=dhidden_layer1\n",
    "    grads['b1'] = torch.sum(dhidden_layer1_output, dim=0)\n",
    "\n",
    "    return loss, grads\n",
    "\n",
    "  def train(self, X, y,\n",
    "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "            num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "    \"\"\"\n",
    "    neural network training using SGD\n",
    "\n",
    "    Inputs:\n",
    "    - X: numpy array of shape (N, D) (training data)\n",
    "    - y: numpy array of shape (N,)(training labels; y[i] = c\n",
    "                                  c is the label of X[i], 0 <= c < C)\n",
    "    - learning_rate: Scalar learning rate\n",
    "    - num_iters: Number of steps\n",
    "    - batch_size: Number of training examples in a mini-batch.\n",
    "    - verbose: if true, print progress\n",
    "    \"\"\"\n",
    "    num_train = X.shape[0]\n",
    "    iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "\n",
    "    # optimization using SGD\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    for it in range(num_iters):\n",
    "      loss, grads = self.loss(X, y=y)\n",
    "      loss_history.append(loss)\n",
    "\n",
    "      \n",
    "      # call gradient in the 'grads' dictionary and carry out SGD update \n",
    "      for key in self.params.keys():\n",
    "            self.params[key]-=learning_rate*grads[key]\n",
    "\n",
    "      if verbose and it % 100 == 0:\n",
    "        print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "\n",
    "      if it % iterations_per_epoch == 0:\n",
    "        # Accuracy\n",
    "        train_acc = (self.predict(X) == y).float().mean()\n",
    "        train_acc_history.append(train_acc)\n",
    "\n",
    "        learning_rate *= learning_rate_decay\n",
    "\n",
    "    return {\n",
    "      'loss_history': loss_history,\n",
    "      'train_acc_history': train_acc_history,\n",
    "      'val_acc_history': val_acc_history,\n",
    "    }\n",
    "\n",
    "  def predict(self, X):\n",
    "    return torch.argmax(self.loss(X),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your scores:\n",
      "tensor([[ 0.2462,  0.1262,  1.1628],\n",
      "        [ 0.1836, -0.0676, -0.2131],\n",
      "        [-0.2075, -0.1253, -0.0651],\n",
      "        [ 0.0864,  0.0717,  0.2353],\n",
      "        [ 0.8220, -0.3256, -0.7781]])\n",
      "\n",
      "correct scores:\n",
      "tensor([[ 0.2462,  0.1262,  1.1628],\n",
      "        [ 0.1836, -0.0676, -0.2131],\n",
      "        [-0.2075, -0.1253, -0.0651],\n",
      "        [ 0.0864,  0.0717,  0.2353],\n",
      "        [ 0.8220, -0.3256, -0.7781]])\n",
      "\n",
      "Difference between your scores and correct scores:\n",
      "tensor(7.4506e-09)\n",
      "Difference between your loss and correct loss:\n",
      "tensor(0.)\n",
      "{'loss_history': [tensor(1.2444), tensor(0.9947), tensor(0.8408), tensor(0.7414), tensor(0.6628), tensor(0.5965), tensor(0.5410), tensor(0.4949), tensor(0.4568), tensor(0.4252), tensor(0.3989), tensor(0.3769), tensor(0.3582), tensor(0.3423), tensor(0.3287), tensor(0.3168), tensor(0.3064), tensor(0.2971), tensor(0.2889), tensor(0.2815), tensor(0.2748), tensor(0.2687), tensor(0.2631), tensor(0.2580), tensor(0.2532), tensor(0.2489), tensor(0.2448), tensor(0.2411), tensor(0.2375), tensor(0.2343), tensor(0.2312), tensor(0.2283), tensor(0.2256), tensor(0.2231), tensor(0.2207), tensor(0.2184), tensor(0.2163), tensor(0.2143), tensor(0.2124), tensor(0.2106), tensor(0.2089), tensor(0.2074), tensor(0.2059), tensor(0.2044), tensor(0.2031), tensor(0.2018), tensor(0.2006), tensor(0.1995), tensor(0.1984), tensor(0.1974), tensor(0.1964), tensor(0.1955), tensor(0.1946), tensor(0.1938), tensor(0.1930), tensor(0.1923), tensor(0.1916), tensor(0.1909), tensor(0.1903), tensor(0.1897), tensor(0.1891), tensor(0.1886), tensor(0.1881), tensor(0.1876), tensor(0.1871), tensor(0.1867), tensor(0.1863), tensor(0.1859), tensor(0.1855), tensor(0.1852), tensor(0.1848), tensor(0.1845), tensor(0.1842), tensor(0.1839), tensor(0.1837), tensor(0.1834), tensor(0.1832), tensor(0.1829), tensor(0.1827), tensor(0.1825), tensor(0.1823), tensor(0.1821), tensor(0.1819), tensor(0.1818), tensor(0.1816), tensor(0.1814), tensor(0.1813), tensor(0.1812), tensor(0.1810), tensor(0.1809), tensor(0.1808), tensor(0.1807), tensor(0.1806), tensor(0.1805), tensor(0.1804), tensor(0.1803), tensor(0.1802), tensor(0.1801), tensor(0.1800), tensor(0.1799)], 'train_acc_history': [tensor(0.4000), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.)], 'val_acc_history': []}\n",
      "Train acc: 0.400000 -> 1.000000\n",
      "Train loss: 1.244415 -> 0.179950\n"
     ]
    }
   ],
   "source": [
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "    torch.manual_seed(0)\n",
    "    return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "    torch.manual_seed(1)\n",
    "    X = 10 * torch.randn(num_inputs, input_size)\n",
    "    y = torch.LongTensor([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()\n",
    "scores = net.loss(X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('correct scores:')\n",
    "correct_scores = torch.Tensor(\n",
    "  [[ 0.24617445,  0.1261572,   1.1627575 ],\n",
    " [ 0.18364899, -0.0675799,  -0.21310908],\n",
    " [-0.2075074,  -0.12525336, -0.06508598],\n",
    " [ 0.08643292,  0.07172455,  0.2353122 ],\n",
    " [ 0.8219606,  -0.32560882, -0.77807254]]\n",
    ")\n",
    "print(correct_scores)\n",
    "print()\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(torch.sum(torch.abs(scores - correct_scores)))\n",
    "loss, _ = net.loss(X, y)\n",
    "correct_loss = 1.2444149\n",
    "\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(torch.sum(torch.abs(loss - correct_loss)))\n",
    "loss, grads = net.loss(X, y)\n",
    "\n",
    "results = net.train(X, y, 0.05)\n",
    "print(results)\n",
    "\n",
    "print(\"Train acc: %f -> %f\\nTrain loss: %f -> %f\" % (results['train_acc_history'][0], results['train_acc_history'][-1]\n",
    "                                                , results['loss_history'][0],results['loss_history'][-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
